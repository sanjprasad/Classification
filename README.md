# Classification
In this notebook, I used the TCGA Risk Factor dataset and the corresponding clinical annotation data. I utilized Excel in order to organize and reformat the data which included pulling the pathologic state variable and matching it to the corresponding samples in order to have the pathologic state associated with each sample in the dataset with all the genes and expression. This was done by hand as I alphabetized the clinical annotation dataset and then matched the corresponding sample name to add in a row called "Stage" which had the pathologic state. As some of the stages only had one sample, I increased the test size parameter to 0.8 in order to have them all show in the resulting graphs be included in the analysis. 

Following the data preparation, I split the data into test and train datasets, and then I used these sets in my classification models. I began with the Support Vector Classifier (SVC) method, which only had an accuracy of about 23%. This was only further visually shown in the classification report graph, which showed very low scores for all the parameters and the PCA plot which showed no distinct grouping. Stage I seemed to have the most success with this method, although it was still rather low as shown by the parameters in the classification report as we saw the highest recall of 0.5, which is not very good for this model. The F1 scores and precision were 0.387 and 0.3157 respectively for stage I. After this, I used feature selection and found the top 5 marker genes to be FTL2512, APOA2336, SEPP16414, CLU1191 and TF7018 with FTL2512 having the highest value of 0.00208.

The second classification method I tried was the Random Forest Classification Method. This method yielded an accuracy of 32.5%, which was much better than the SVC method. This was shown by the classification report where we can see that the recall for Stage 1 is high at almost 0.916666 and the precision is at 0.3928. The recall is good for this class as it's above 0.5, but the precision is still pretty low. The F1 score is also decently good at about 0.55 as well for this class. Thus, the model showed the most success with Stage I as well. The confusion matrix showed that 10 of the samples were correctly classified with this method. Feature selection was done again with the five marker genes bein ZWINT11130, TAF383860, MRPL965005, PRCC5546, STK2510494, with the corresponding values of 0.0038, 0.0033, 0.0032,0.0030, 0.0027.

Interestingly, the significant features were different for both models, but since neither model had a high accuracy it's hard to determine which is more correct. 

Lastly, I used the Lazy Predict classifier method to look at many of the models that would allow comparing of time taken, F1 scores and accuracy. The top models found were DecisionTreeClassifier, LinearDiscriminantAnalysis, AdaBoostClassifier, KNeighborsClassifer, and SVC. It's interesting that SVC was found to be on this list, considering the accuracy was lower than the Random Forest method. While AdaBoostClassifier was found to be the fastest, it had a low F1 score. The highest F1 score belonged to the Decision Tree Classifier method. As we had discussed in class, F1 scores are a significant parameter to look at and more important than speed, so the Decsion Tree classifier is a model of interest to look at. Interestingly, the accuracy of SVC was fairly high which was different than seen here. 
